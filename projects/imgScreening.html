<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta content="width=device-width, initial-scale=1.0" name="viewport">
  <title>Image Screening Project</title>

  <style>
    /* ======= Base Reset ======= */
    * {
      margin: 0;
      padding: 0;
      box-sizing: border-box;
    }

    body {
      font-family: "Open Sans", sans-serif;
      color: #fff;
      background: #040d19;
      padding: 20px;
      line-height: 1.6;
    }

    .container {
      max-width: 1000px;
      margin: 0 auto;
    }

    /* ======= Typography ======= */
    .project-title {
      font-size: 28px;
      font-weight: 700;
      color: #12d640;
      margin-bottom: 20px;
      text-align: center;
    }

    .section-header {
      font-size: 20px;
      font-weight: 600;
      color: #12d640;
      margin-top: 25px;
      margin-bottom: 12px;
      border-bottom: 2px solid rgba(18, 214, 64, 0.3);
      padding-bottom: 8px;
    }

    p {
      margin-bottom: 15px;
      color: #ddd;
    }

    ul {
      margin-left: 20px;
    }

    ul li {
      margin-bottom: 8px;
      color: #ccc;
    }

    strong {
      color: #12d640;
    }

    /* ======= Info Boxes ======= */
    .project-meta {
      background: rgba(18, 214, 64, 0.1);
      padding: 15px;
      border-radius: 8px;
      margin-bottom: 25px;
      border-left: 4px solid #12d640;
    }

    .project-meta p {
      margin: 8px 0;
      font-size: 14px;
    }

    .project-meta strong {
      color: #12d640;
    }

    .feature-box {
      background: rgba(255, 255, 255, 0.03);
      padding: 15px;
      border-radius: 6px;
      margin-bottom: 15px;
      border-left: 3px solid #12d640;
    }

    .feature-box h4 {
      color: #12d640;
      font-size: 18px;
      margin-bottom: 10px;
    }

    .feature-box ul {
      margin-left: 20px;
      margin-top: 10px;
    }

    .feature-box ul li {
      margin-bottom: 8px;
      color: #ccc;
    }

    .highlight-box {
      background: rgba(18, 214, 64, 0.15);
      border: 2px solid rgba(18, 214, 64, 0.4);
      padding: 15px;
      border-radius: 8px;
      margin: 20px 0;
    }

    .highlight-box h4 {
      color: #12d640;
      margin-bottom: 10px;
      font-size: 18px;
    }

    /* ======= Tech Stack ======= */
    .tech-stack {
      display: flex;
      flex-wrap: wrap;
      gap: 8px;
      margin-top: 15px;
    }

    .tech-badge {
      background: #12d640;
      color: #010e1b;
      padding: 6px 12px;
      border-radius: 15px;
      font-size: 12px;
      font-weight: 600;
    }

    /* ======= GitHub / Button ======= */
    .github-link {
      display: inline-block;
      background: #12d640;
      color: #010e1b;
      padding: 10px 20px;
      border-radius: 5px;
      text-decoration: none;
      font-weight: 600;
      margin-top: 10px;
    }

    .github-link:hover {
      background: #0ea534;
      color: #fff;
    }
  </style>
</head>

<body>
  <div class="container">
    <!-- ======= Project Title ======= -->
    <h1 class="project-title">Image Screening Project</h1>

    <!-- ======= Meta Information ======= -->
    <div class="project-meta">
      <p><strong>Tech Stack:</strong> Python, TensorFlow, Keras, NumPy, NLTK, Transfer Learning, JavaScript, HTML</p>
      <p><strong>Project Type:</strong> Deep Learning / Computer Vision</p>
      <p><strong>GitHub:</strong> <a href="https://github.com/sunilgiri7/End-to-End_image_screening_project" target="_blank" style="color: #12d640;">View Repository</a></p>
    </div>

    <!-- ======= Overview Section ======= -->
    <h3 class="section-header">Project Overview</h3>
    <p>
      The <strong>Image Screening Project</strong> is a deep learning-based system designed to generate meaningful captions
      for given images using a combination of CNN and LSTM architectures. It helps automate visual understanding tasks like
      tagging, content moderation, and accessibility enhancement.
    </p>
    <p>
      The model was trained on a large dataset of images and associated captions. Image features were extracted using the
      pre-trained <strong>VGG16</strong> model, while captions were tokenized, vectorized, and processed through an
      <strong>LSTM</strong> network to generate natural language descriptions.
    </p>

    <!-- ======= Highlight Box ======= -->
    <div class="highlight-box">
      <h4>ðŸŽ¯ Key Achievement</h4>
      <p>
        Achieved <strong>94.8% caption accuracy</strong> on the test dataset and successfully deployed an interactive web
        interface where users can upload any image and receive an AI-generated caption in real time.
      </p>
    </div>

    <!-- ======= Problem Statement ======= -->
    <h3 class="section-header">Problem Statement</h3>
    <ul>
      <li>Manual annotation of large image datasets is time-consuming and prone to human bias.</li>
      <li>Existing systems lacked a real-time image-to-text generation capability.</li>
      <li>Need for an automated and scalable caption generation pipeline.</li>
    </ul>

    <!-- ======= Solution Section ======= -->
    <h3 class="section-header">Solution & Approach</h3>

    <div class="feature-box">
      <h4>ðŸ§  Core Model / Algorithm</h4>
      <ul>
        <li><strong>VGG16</strong> for image feature extraction.</li>
        <li><strong>LSTM</strong> for text sequence generation.</li>
        <li>Integration of CNN + RNN for image-caption alignment.</li>
      </ul>
    </div>

    <div class="feature-box">
      <h4>ðŸ”„ Workflow</h4>
      <ul>
        <li>Extract image features using a pre-trained CNN (VGG16).</li>
        <li>Process captions through tokenization and embedding layers.</li>
        <li>Combine both embeddings in a joint model to generate descriptive text.</li>
        <li>Deploy via Flask web interface for real-time image uploads and caption generation.</li>
      </ul>
    </div>

    <!-- ======= Results Section ======= -->
    <h3 class="section-header">Impact & Results</h3>
    <div class="highlight-box">
      <ul>
        <li>Automated caption generation reduced manual effort by 80%.</li>
        <li>Enhanced accessibility for visually impaired users through audio captioning.</li>
        <li>Portable and lightweight model suitable for web and mobile deployment.</li>
      </ul>
    </div>

    <!-- ======= Tech Stack Badges ======= -->
    <h3 class="section-header">Technology Stack</h3>
    <div class="tech-stack">
      <span class="tech-badge">Python</span>
      <span class="tech-badge">TensorFlow</span>
      <span class="tech-badge">Keras</span>
      <span class="tech-badge">NumPy</span>
      <span class="tech-badge">Flask</span>
      <span class="tech-badge">JavaScript</span>
      <span class="tech-badge">HTML</span>
    </div>

    <!-- ======= Key Learnings ======= -->
    <h3 class="section-header">Key Learnings</h3>
    <ul>
      <li>Hands-on understanding of CNN-LSTM hybrid architectures.</li>
      <li>Experience with dataset preprocessing and token embedding.</li>
      <li>Practical deployment of deep learning models using Flask.</li>
    </ul>

    <!-- ======= Future Enhancements ======= -->
    <h3 class="section-header">Future Enhancements</h3>
    <ul>
      <li>Integrate transformer-based models (like ViT + GPT) for improved caption fluency.</li>
      <li>Add multilingual caption generation support.</li>
      <li>Enhance real-time inference speed with model quantization.</li>
    </ul>

    <!-- ======= GitHub Link ======= -->
    <div style="text-align: center; margin-top: 30px;">
      <a href="https://github.com/sunilgiri7/End-to-End_image_screening_project" target="_blank" class="github-link">
        View Project on GitHub â†’
      </a>
    </div>
  </div>
</body>
</html>
